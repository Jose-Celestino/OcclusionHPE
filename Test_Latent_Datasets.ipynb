{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import sys, os, natsort, glob\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch, datasets\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "\n",
    "import mediapipe as mp\n",
    "import utils\n",
    "import LantentNet\n",
    "cudnn.enabled = True\n",
    "\n",
    "gpu = 0 # GPU ID\n",
    "\n",
    "#Initialize Model\n",
    "model = LantentNet.LantentNet(torchvision.models.resnet.Bottleneck, [3, 4, 6, 3], 66)\n",
    "saved_state_dict = torch.load('models\\Latent_model_0,999.pkl') # 0,999 model for best yaw results\n",
    "model.load_state_dict(saved_state_dict)\n",
    "\n",
    "\n",
    "\n",
    "def load_filtered_state_dict(model, snapshot):\n",
    "    # By user apaszke from discuss.pytorch.org\n",
    "    model_dict = model.state_dict()\n",
    "    snapshot = {k: v for k, v in snapshot.items() if k in model_dict}\n",
    "    model_dict.update(snapshot)\n",
    "    model.load_state_dict(model_dict)\n",
    "\n",
    "\n",
    "#Classification and Regression losses\n",
    "criterion = nn.CrossEntropyLoss().cuda(gpu)\n",
    "reg_criterion = nn.MSELoss().cuda(gpu)\n",
    "\n",
    "\n",
    "MAE = torch.nn.L1Loss(reduction='mean')\n",
    "\n",
    "\n",
    "softmax = nn.Softmax().cuda(gpu)\n",
    "\n",
    "#For classification vector\n",
    "idx_tensor = [idx for idx in range(66)]\n",
    "idx_tensor = Variable(torch.FloatTensor(idx_tensor)).cuda(gpu)\n",
    "\n",
    "\n",
    "model.cuda(gpu)\n",
    "model.eval() #Inference mode\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AFLW2000 OCCLUDED DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
     ]
    }
   ],
   "source": [
    "image_path = natsort.natsorted(glob.glob('datasets\\Data_Ocluded_AFLW2000_Full_Faces\\*.jpg'))\n",
    "\n",
    "pose_path = natsort.natsorted(glob.glob('datasets\\AFLW2000-3D\\AFLW2000\\*.mat'))\n",
    "\n",
    "transformations =  transforms.Compose([transforms.Resize(260), transforms.CenterCrop(224), transforms.ToTensor(),transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "test_dataset = datasets.new_Pose_AFLW2000(image_path, pose_path, transformations)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,batch_size=1,shuffle=False, num_workers=0)\n",
    "print(len(image_path))\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST OCCLUDED AFLW2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
     ]
    }
   ],
   "source": [
    "MAE_LATENT_y = np.array([])\n",
    "MAE_LATENT_p = np.array([])\n",
    "MAE_LATENT_r = np.array([])\n",
    "\n",
    "\n",
    "num_epochs = 1\n",
    "with torch.no_grad():\n",
    "    for i, (img, labels, cont_labels) in enumerate(test_loader):\n",
    "\n",
    "        img = Variable(img).cuda(gpu)\n",
    "\n",
    "        label_yaw_cont = Variable(cont_labels[:,0]).cuda(gpu)\n",
    "        label_pitch_cont = Variable(cont_labels[:,1]).cuda(gpu)\n",
    "        label_roll_cont = Variable(cont_labels[:,2]).cuda(gpu)\n",
    "\n",
    "        if abs(label_yaw_cont) > 99 or abs(label_pitch_cont) > 99 or abs(label_roll_cont) > 99:\n",
    "            #print('out of range')\n",
    "            continue\n",
    "      \n",
    "        x, yaw, pitch, roll = model(img)\n",
    "\n",
    "        yaw_predicted = softmax(yaw)#,dim = 1)\n",
    "        pitch_predicted = softmax(pitch)#,dim = 1)\n",
    "        roll_predicted = softmax(roll)#,dim =1)\n",
    "\n",
    "        yaw_predicted = torch.sum(yaw_predicted * idx_tensor, 1) * 3 - 99\n",
    "        pitch_predicted = torch.sum(pitch_predicted * idx_tensor, 1) * 3 - 99\n",
    "        roll_predicted = torch.sum(roll_predicted * idx_tensor, 1) * 3 - 99\n",
    "\n",
    "        yaw_predicted = yaw_predicted.view(1)\n",
    "        pitch_predicted = pitch_predicted.view(1)\n",
    "        roll_predicted = roll_predicted.view(1)\n",
    "\n",
    "        err_y = MAE(yaw_predicted,label_yaw_cont)\n",
    "        err_p = MAE(pitch_predicted,label_pitch_cont)\n",
    "        err_r = MAE(roll_predicted,label_roll_cont)\n",
    "\n",
    "        MAE_LATENT_y = np.append(MAE_LATENT_y,err_y.cpu())\n",
    "        MAE_LATENT_p = np.append(MAE_LATENT_p,err_p.cpu())\n",
    "        MAE_LATENT_r = np.append(MAE_LATENT_r,err_r.cpu())\n",
    "\n",
    "        if (i%1000 == 0):\n",
    "            print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE ERROR LATENT OCCLUSION AFWL2000:\n",
      "Yaw:  4.644780082663132  Pitch:  6.165434748736537  Roll:  4.746627412617221\n"
     ]
    }
   ],
   "source": [
    "print('MAE ERROR LATENT OCCLUSION AFWL2000:')\n",
    "print('Yaw: ',sum(MAE_LATENT_y)/len(MAE_LATENT_y), ' Pitch: ',sum(MAE_LATENT_p)/len(MAE_LATENT_y), ' Roll: ',sum(MAE_LATENT_r)/len(MAE_LATENT_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIWI OCCLUDED DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
     ]
    }
   ],
   "source": [
    "image_path = natsort.natsorted(glob.glob('datasets\\faces_biwi_wide_oclusion\\*.jpg'),reverse= False)\n",
    "\n",
    "\n",
    "pose_path = natsort.natsorted(glob.glob('datasets\\faces_biwi\\01\\*.txt'),reverse= False) + natsort.natsorted(glob.glob('datasets\\faces_biwi\\02\\*.txt'),reverse= False) + natsort.natsorted(glob.glob('datasets\\faces_biwi\\03\\*.txt'),reverse= False) + natsort.natsorted(glob.glob('datasets\\faces_biwi\\04\\*.txt'),reverse= False)+ natsort.natsorted(glob.glob('datasets\\faces_biwi\\05\\*.txt'),reverse= False)\n",
    "\n",
    "\n",
    "transformations = transforms.Compose([transforms.Resize(224), transforms.CenterCrop(224), transforms.ToTensor(),transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "\n",
    "\n",
    "biwi_test_dataset = datasets.new_BIWI(image_path,pose_path, transformations)\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=biwi_test_dataset,\n",
    "                                               batch_size=1,\n",
    "                                               shuffle=False,\n",
    "                                               num_workers=0)\n",
    "print(len(image_path))\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIWI DATASET TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
     ]
    }
   ],
   "source": [
    "MAE_LATENT_y = np.array([])\n",
    "MAE_LATENT_p = np.array([])\n",
    "MAE_LATENT_r = np.array([])\n",
    "\n",
    "\n",
    "num_epochs = 1\n",
    "with torch.no_grad():\n",
    "    for i, (img, labels, cont_labels) in enumerate(test_loader):\n",
    "\n",
    "        img = Variable(img).cuda(gpu)\n",
    "\n",
    "        label_yaw_cont = Variable(cont_labels[:,0]).cuda(gpu)\n",
    "        label_pitch_cont = Variable(cont_labels[:,1]).cuda(gpu)\n",
    "        label_roll_cont = Variable(cont_labels[:,2]).cuda(gpu)\n",
    "\n",
    "        if abs(label_yaw_cont) > 99 or abs(label_pitch_cont) > 99 or abs(label_roll_cont) > 99:\n",
    "            #print('out of range')\n",
    "            continue\n",
    "      \n",
    "        x, yaw, pitch, roll = model(img)\n",
    "\n",
    "        yaw_predicted = softmax(yaw)#,dim = 1)\n",
    "        pitch_predicted = softmax(pitch)#,dim = 1)\n",
    "        roll_predicted = softmax(roll)#,dim =1)\n",
    "\n",
    "        yaw_predicted = torch.sum(yaw_predicted * idx_tensor, 1) * 3 - 99\n",
    "        pitch_predicted = torch.sum(pitch_predicted * idx_tensor, 1) * 3 - 99\n",
    "        roll_predicted = torch.sum(roll_predicted * idx_tensor, 1) * 3 - 99\n",
    "\n",
    "        yaw_predicted = yaw_predicted.view(1)\n",
    "        pitch_predicted = pitch_predicted.view(1)\n",
    "        roll_predicted = roll_predicted.view(1)\n",
    "\n",
    "        err_y = MAE(yaw_predicted,label_yaw_cont)\n",
    "        err_p = MAE(pitch_predicted,label_pitch_cont)\n",
    "        err_r = MAE(roll_predicted,label_roll_cont)\n",
    "\n",
    "        MAE_LATENT_y = np.append(MAE_LATENT_y,err_y.cpu())\n",
    "        MAE_LATENT_p = np.append(MAE_LATENT_p,err_p.cpu())\n",
    "        MAE_LATENT_r = np.append(MAE_LATENT_r,err_r.cpu())\n",
    "\n",
    "        if (i%1000 == 0):\n",
    "            print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE ERROR LATENT BIWI:\n",
      "Yaw:  5.174480246811149  Pitch:  6.622005143735569  Roll:  4.116956870948608\n"
     ]
    }
   ],
   "source": [
    "print('MAE ERROR LATENT OCCLUSION BIWI:')\n",
    "print('Yaw: ',sum(MAE_LATENT_y)/len(MAE_LATENT_y), ' Pitch: ',sum(MAE_LATENT_p)/len(MAE_LATENT_y), ' Roll: ',sum(MAE_LATENT_r)/len(MAE_LATENT_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AFLW2000 NO OCCLUSION TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "image_path = natsort.natsorted(glob.glob('datasets\\AFLW2000_3D_Full_Faces\\*.jpg'))\n",
    "\n",
    "pose_path = natsort.natsorted(glob.glob('datasets\\AFLW2000-3D\\AFLW2000\\*.mat'))\n",
    "\n",
    "transformations =  transforms.Compose([transforms.Resize(260), transforms.CenterCrop(224), transforms.ToTensor(),transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "test_dataset = datasets.new_Pose_AFLW2000(image_path, pose_path, transformations)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,batch_size=1,shuffle=False, num_workers=0)\n",
    "print(len(image_path))\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
     ]
    }
   ],
   "source": [
    "MAE_LATENT_y = np.array([])\n",
    "MAE_LATENT_p = np.array([])\n",
    "MAE_LATENT_r = np.array([])\n",
    "\n",
    "\n",
    "num_epochs = 1\n",
    "with torch.no_grad():\n",
    "    for i, (img, labels, cont_labels) in enumerate(test_loader):\n",
    "\n",
    "        img = Variable(img).cuda(gpu)\n",
    "\n",
    "        label_yaw_cont = Variable(cont_labels[:,0]).cuda(gpu)\n",
    "        label_pitch_cont = Variable(cont_labels[:,1]).cuda(gpu)\n",
    "        label_roll_cont = Variable(cont_labels[:,2]).cuda(gpu)\n",
    "\n",
    "        if abs(label_yaw_cont) > 99 or abs(label_pitch_cont) > 99 or abs(label_roll_cont) > 99:\n",
    "            #print('out of range')\n",
    "            continue\n",
    "      \n",
    "        x, yaw, pitch, roll = model(img)\n",
    "\n",
    "        yaw_predicted = softmax(yaw)#,dim = 1)\n",
    "        pitch_predicted = softmax(pitch)#,dim = 1)\n",
    "        roll_predicted = softmax(roll)#,dim =1)\n",
    "\n",
    "        yaw_predicted = torch.sum(yaw_predicted * idx_tensor, 1) * 3 - 99\n",
    "        pitch_predicted = torch.sum(pitch_predicted * idx_tensor, 1) * 3 - 99\n",
    "        roll_predicted = torch.sum(roll_predicted * idx_tensor, 1) * 3 - 99\n",
    "\n",
    "        yaw_predicted = yaw_predicted.view(1)\n",
    "        pitch_predicted = pitch_predicted.view(1)\n",
    "        roll_predicted = roll_predicted.view(1)\n",
    "\n",
    "        err_y = MAE(yaw_predicted,label_yaw_cont)\n",
    "        err_p = MAE(pitch_predicted,label_pitch_cont)\n",
    "        err_r = MAE(roll_predicted,label_roll_cont)\n",
    "\n",
    "        MAE_LATENT_y = np.append(MAE_LATENT_y,err_y.cpu())\n",
    "        MAE_LATENT_p = np.append(MAE_LATENT_p,err_p.cpu())\n",
    "        MAE_LATENT_r = np.append(MAE_LATENT_r,err_r.cpu())\n",
    "\n",
    "        if (i%1000 == 0):\n",
    "            print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE ERROR LATENT NO OCCLUSION AFLW2000:\n",
      "Yaw:  3.7374907772687256  Pitch:  5.415608577725395  Roll:  3.9669136630022144\n"
     ]
    }
   ],
   "source": [
    "print('MAE ERROR LATENT NO OCCLUSION AFLW2000:')\n",
    "print('Yaw: ',sum(MAE_LATENT_y)/len(MAE_LATENT_y), ' Pitch: ',sum(MAE_LATENT_p)/len(MAE_LATENT_y), ' Roll: ',sum(MAE_LATENT_r)/len(MAE_LATENT_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIWI NO OCCLUSION TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3430\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "image_path = natsort.natsorted(glob.glob('datasets\\faces_biwi_wide\\*.jpg'),reverse= False)\n",
    "\n",
    "pose_path = natsort.natsorted(glob.glob('datasets\\faces_biwi\\01\\*.txt'),reverse= False) + natsort.natsorted(glob.glob('datasets\\faces_biwi\\02\\*.txt'),reverse= False) + natsort.natsorted(glob.glob('datasets\\faces_biwi\\03\\*.txt'),reverse= False) + natsort.natsorted(glob.glob('datasets\\faces_biwi\\04\\*.txt'),reverse= False)+ natsort.natsorted(glob.glob('datasets\\faces_biwi\\05\\*.txt'),reverse= False)\n",
    "\n",
    "\n",
    "transformations = transforms.Compose([transforms.Resize(224), transforms.CenterCrop(224), transforms.ToTensor(),transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "\n",
    "biwi_test_dataset = datasets.new_BIWI(image_path,pose_path, transformations)\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=biwi_test_dataset,\n",
    "                                               batch_size=1,\n",
    "                                               shuffle=False,\n",
    "                                               num_workers=0)\n",
    "print(len(image_path))\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zecar\\AppData\\Local\\Temp/ipykernel_31492/24992921.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  yaw_predicted = softmax(yaw)#,dim = 1)\n",
      "C:\\Users\\zecar\\AppData\\Local\\Temp/ipykernel_31492/24992921.py:23: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pitch_predicted = softmax(pitch)#,dim = 1)\n",
      "C:\\Users\\zecar\\AppData\\Local\\Temp/ipykernel_31492/24992921.py:24: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  roll_predicted = softmax(roll)#,dim =1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n"
     ]
    }
   ],
   "source": [
    "MAE_LATENT_y = np.array([])\n",
    "MAE_LATENT_p = np.array([])\n",
    "MAE_LATENT_r = np.array([])\n",
    "\n",
    "\n",
    "num_epochs = 1\n",
    "with torch.no_grad():\n",
    "    for i, (img, labels, cont_labels) in enumerate(test_loader):\n",
    "\n",
    "        img = Variable(img).cuda(gpu)\n",
    "\n",
    "        label_yaw_cont = Variable(cont_labels[:,0]).cuda(gpu)\n",
    "        label_pitch_cont = Variable(cont_labels[:,1]).cuda(gpu)\n",
    "        label_roll_cont = Variable(cont_labels[:,2]).cuda(gpu)\n",
    "\n",
    "        if abs(label_yaw_cont) > 99 or abs(label_pitch_cont) > 99 or abs(label_roll_cont) > 99:\n",
    "            #print('out of range')\n",
    "            continue\n",
    "      \n",
    "        x, yaw, pitch, roll = model(img)\n",
    "\n",
    "        yaw_predicted = softmax(yaw)#,dim = 1)\n",
    "        pitch_predicted = softmax(pitch)#,dim = 1)\n",
    "        roll_predicted = softmax(roll)#,dim =1)\n",
    "\n",
    "        yaw_predicted = torch.sum(yaw_predicted * idx_tensor, 1) * 3 - 99\n",
    "        pitch_predicted = torch.sum(pitch_predicted * idx_tensor, 1) * 3 - 99\n",
    "        roll_predicted = torch.sum(roll_predicted * idx_tensor, 1) * 3 - 99\n",
    "\n",
    "        yaw_predicted = yaw_predicted.view(1)\n",
    "        pitch_predicted = pitch_predicted.view(1)\n",
    "        roll_predicted = roll_predicted.view(1)\n",
    "\n",
    "        err_y = MAE(yaw_predicted,label_yaw_cont)\n",
    "        err_p = MAE(pitch_predicted,label_pitch_cont)\n",
    "        err_r = MAE(roll_predicted,label_roll_cont)\n",
    "\n",
    "        MAE_LATENT_y = np.append(MAE_LATENT_y,err_y.cpu())\n",
    "        MAE_LATENT_p = np.append(MAE_LATENT_p,err_p.cpu())\n",
    "        MAE_LATENT_r = np.append(MAE_LATENT_r,err_r.cpu())\n",
    "\n",
    "        if (i%1000 == 0):\n",
    "            print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE ERROR LATENT NO OCCLUSION BIWI:\n",
      "Yaw:  4.296622778452477  Pitch:  4.586208105234989  Roll:  3.617039893832568\n"
     ]
    }
   ],
   "source": [
    "print('MAE ERROR LATENT NO OCCLUSION BIWI:')\n",
    "print('Yaw: ',sum(MAE_LATENT_y)/len(MAE_LATENT_y), ' Pitch: ',sum(MAE_LATENT_p)/len(MAE_LATENT_y), ' Roll: ',sum(MAE_LATENT_r)/len(MAE_LATENT_y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b656e7a5427a8187cfadc7f7a5303afbaa9a05fff8d95051e844a826c0af12e2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
