{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, natsort, glob\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch, datasets\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "\n",
    "import mediapipe as mp\n",
    "import utils\n",
    "import LantentNet\n",
    "cudnn.enabled = True\n",
    "\n",
    "gpu = 0 # GPU ID\n",
    "\n",
    "#Initialize Model\n",
    "model = LantentNet.LantentNet(torchvision.models.resnet.Bottleneck, [3, 4, 6, 3], 66)\n",
    "#saved_state_dict = torch.load('models\\Latent_model_0.pkl')\n",
    "#saved_state_dict = torch.load('models\\Latent_model_0,5.pkl')\n",
    "#saved_state_dict = torch.load('models\\Latent_model_0,990.pkl')\n",
    "saved_state_dict = torch.load('models\\Latent_model_0,999.pkl') #For best yaw results\n",
    "#saved_state_dict = torch.load('models\\Latent_model_1.pkl')\n",
    "model.load_state_dict(saved_state_dict)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Classification and Regression losses\n",
    "criterion = nn.CrossEntropyLoss().cuda(gpu)\n",
    "reg_criterion = nn.MSELoss().cuda(gpu)\n",
    "\n",
    "\n",
    "MAE = torch.nn.L1Loss(reduction='mean')\n",
    "\n",
    "\n",
    "softmax = nn.Softmax().cuda(gpu)\n",
    "\n",
    "#For classification vector\n",
    "idx_tensor = [idx for idx in range(66)]\n",
    "idx_tensor = Variable(torch.FloatTensor(idx_tensor)).cuda(gpu)\n",
    "\n",
    "\n",
    "model.cuda(gpu)\n",
    "model.eval() #Inference mode\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 300W-LP DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = natsort.natsorted(glob.glob('datasets\\300W-LP_Full\\*.jpg'),reverse= False)\n",
    "image_path = natsort.natsorted(glob.glob('datasets\\Data_Ocluded_300WLP_FULL_25_2\\*.jpg'),reverse= False)\n",
    "\n",
    "transformations = transforms.Compose([transforms.Resize(240), transforms.CenterCrop(224), transforms.ToTensor(),transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "d_mat1 =  natsort.natsorted(glob.glob('datasets\\300W-LP\\300W_LP\\AFW\\*.mat'),reverse= False) \n",
    "d_mat2 = natsort.natsorted(glob.glob('datasets\\300W-LP\\300W_LP\\HELEN\\*.mat'),reverse= False) \n",
    "d_mat3 = natsort.natsorted(glob.glob('datasets\\300W-LP\\300W_LP\\IBUG\\*.mat'),reverse= False) \n",
    "d_mat4 = natsort.natsorted(glob.glob('datasets\\300W-LP\\300W_LP\\LFPW\\*.mat'),reverse= False)\n",
    "\n",
    "pose_path = d_mat1+d_mat2+d_mat3+d_mat4\n",
    "\n",
    "\n",
    "pose_dataset = datasets.o_new_Pose_300W_LP_test(image_path, pose_path, transformations)\n",
    "\n",
    "batch_size = 1\n",
    "test_loader = torch.utils.data.DataLoader(dataset=pose_dataset,\n",
    "                                               batch_size=batch_size,\n",
    "                                               shuffle=False,\n",
    "                                               num_workers=0)\n",
    "\n",
    "print(len(image_path))\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIWI DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = natsort.natsorted(glob.glob('datasets\\faces_biwi_wide_oclusion\\*.jpg'),reverse= False)\n",
    "image_path = natsort.natsorted(glob.glob('datasets\\faces_biwi_wide\\*.jpg'),reverse= False)\n",
    "\n",
    "\n",
    "pose_path = natsort.natsorted(glob.glob('datasets\\faces_biwi\\01\\*.txt'),reverse= False) + natsort.natsorted(glob.glob('datasets\\faces_biwi\\02\\*.txt'),reverse= False) + natsort.natsorted(glob.glob('datasets\\faces_biwi\\03\\*.txt'),reverse= False) + natsort.natsorted(glob.glob('datasets\\faces_biwi\\04\\*.txt'),reverse= False)+ natsort.natsorted(glob.glob('datasets\\faces_biwi\\05\\*.txt'),reverse= False)\n",
    "\n",
    "\n",
    "transformations = transforms.Compose([transforms.Resize(224), transforms.CenterCrop(224), transforms.ToTensor(),transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "\n",
    "\n",
    "biwi_test_dataset = datasets.new_BIWI(image_path,pose_path, transformations)\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=biwi_test_dataset,\n",
    "                                               batch_size=1,\n",
    "                                               shuffle=False,\n",
    "                                               num_workers=0)\n",
    "print(len(image_path))\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AFLW2000 DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = natsort.natsorted(glob.glob('datasets\\AFLW2000_3D_Full_Faces\\*.jpg'))\n",
    "image_path = natsort.natsorted(glob.glob('datasets\\Data_Ocluded_AFLW2000_Full_Faces\\*.jpg'))\n",
    "\n",
    "pose_path = natsort.natsorted(glob.glob('datasets\\AFLW2000-3D\\AFLW2000\\*.mat'))\n",
    "\n",
    "transformations =  transforms.Compose([transforms.Resize(224), transforms.CenterCrop(224), transforms.ToTensor(),transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "test_dataset = datasets.new_Pose_AFLW2000(image_path, pose_path, transformations)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,batch_size=1,shuffle=False, num_workers=0)\n",
    "print(len(image_path))\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAE_LATENT_y = np.array([])\n",
    "MAE_LATENT_p = np.array([])\n",
    "MAE_LATENT_r = np.array([])\n",
    "\n",
    "\n",
    "num_epochs = 1\n",
    "with torch.no_grad():\n",
    "    for i, (img, labels, cont_labels) in enumerate(test_loader):\n",
    "\n",
    "        img = Variable(img).cuda(gpu)\n",
    "\n",
    "        label_yaw_cont = Variable(cont_labels[:,0]).cuda(gpu)\n",
    "        label_pitch_cont = Variable(cont_labels[:,1]).cuda(gpu)\n",
    "        label_roll_cont = Variable(cont_labels[:,2]).cuda(gpu)\n",
    "      \n",
    "        if abs(label_yaw_cont) > 99 or abs(label_pitch_cont) > 99 or abs(label_roll_cont) > 99:\n",
    "            continue\n",
    "        \n",
    "        x, yaw, pitch, roll = model(img)\n",
    "\n",
    "        yaw_predicted = softmax(yaw)#,dim = 1)\n",
    "        pitch_predicted = softmax(pitch)#,dim = 1)\n",
    "        roll_predicted = softmax(roll)#,dim =1)\n",
    "\n",
    "        yaw_predicted = torch.sum(yaw_predicted * idx_tensor, 1) * 3 - 99\n",
    "        pitch_predicted = torch.sum(pitch_predicted * idx_tensor, 1) * 3 - 99\n",
    "        roll_predicted = torch.sum(roll_predicted * idx_tensor, 1) * 3 - 99\n",
    "\n",
    "        yaw_predicted = yaw_predicted.view(1)\n",
    "        pitch_predicted = pitch_predicted.view(1)\n",
    "        roll_predicted = roll_predicted.view(1)\n",
    "\n",
    "        err_y = MAE(yaw_predicted,label_yaw_cont)\n",
    "        err_p = MAE(pitch_predicted,label_pitch_cont)\n",
    "        err_r = MAE(roll_predicted,label_roll_cont)\n",
    "\n",
    "        MAE_LATENT_y = np.append(MAE_LATENT_y,err_y.cpu())\n",
    "        MAE_LATENT_p = np.append(MAE_LATENT_p,err_p.cpu())\n",
    "        MAE_LATENT_r = np.append(MAE_LATENT_r,err_r.cpu())\n",
    "\n",
    "        if (i%1000 == 0):\n",
    "            print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MAE ERROR LATENT:')\n",
    "print('Yaw: ',sum(MAE_LATENT_y)/len(MAE_LATENT_y), ' Pitch: ',sum(MAE_LATENT_p)/len(MAE_LATENT_y), ' Roll: ',sum(MAE_LATENT_r)/len(MAE_LATENT_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFERENCE FOR INDIVIDUAL FACE IMAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input face image \n",
    "img = Image.open('wheel2.jpg')\n",
    "frame = cv2.imread('wheel2.jpg')\n",
    "frame = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n",
    "\n",
    "transformations = transforms.Compose([transforms.Resize(224), transforms.CenterCrop(224), transforms.ToTensor(),transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "image = transformations(img)\n",
    "\n",
    "\n",
    "image = image.view(1, 3, 224, 224) \n",
    "image = Variable(image).cuda(gpu)\n",
    "x, yaw, pitch, roll = model(image)\n",
    "\n",
    "yaw_predicted = softmax(yaw)#,dim = 1)\n",
    "pitch_predicted = softmax(pitch)#,dim = 1)\n",
    "roll_predicted = softmax(roll)#,dim =1)\n",
    "\n",
    "yaw_predicted = torch.sum(yaw_predicted * idx_tensor, 1) * 3 - 99\n",
    "pitch_predicted = torch.sum(pitch_predicted * idx_tensor, 1) * 3 - 99\n",
    "roll_predicted = torch.sum(roll_predicted * idx_tensor, 1) * 3 - 99\n",
    "\n",
    "yaw_predicted = yaw_predicted.view(1)\n",
    "pitch_predicted = pitch_predicted.view(1)\n",
    "roll_predicted = roll_predicted.view(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import utils\n",
    "\n",
    "#Visualize result\n",
    "utils.draw_axis(frame, yaw_predicted, pitch_predicted, roll_predicted, tdx = frame.shape[1] / 2, tdy= frame.shape[0] / 2, size = frame.shape[1]/2)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFERENCE FOR INDIVIDUAL IMAGES WITH FACE DETECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "\n",
    "mpFaceDetection = mp.solutions.face_detection\n",
    "mpDraw = mp.solutions.drawing_utils\n",
    "faceDetection = mpFaceDetection.FaceDetection(min_detection_confidence=0.5) #0.65\n",
    "\n",
    "\n",
    "#input face image \n",
    "#img = Image.open('occlusion.jpg')\n",
    "#frame = cv2.imread('occlusion.jpg')\n",
    "img = Image.open('wheel2.jpg')\n",
    "frame = cv2.imread('wheel2.jpg')\n",
    "frame = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n",
    "\n",
    "results = faceDetection.process(frame)\n",
    "\n",
    "detection = results.detections[0]\n",
    "\n",
    "xmin = detection.location_data.relative_bounding_box.xmin\n",
    "xmax = xmin + detection.location_data.relative_bounding_box.width\n",
    "ymin = detection.location_data.relative_bounding_box.ymin\n",
    "ymax = ymin + detection.location_data.relative_bounding_box.height\n",
    "\n",
    "height = frame.shape[0]\n",
    "width = frame.shape[1]\n",
    "\n",
    "bbox_height = ymax-ymin\n",
    "\n",
    "imgRGB = Image.fromarray(frame)\n",
    "\n",
    "xmin_new = xmin*width \n",
    "ymin_new = ymin*height\n",
    "xmax_new = xmax*width\n",
    "ymax_new = ymax*height\n",
    "\n",
    "# widen the face box margin for better pose estimation\n",
    "xmin_new = xmin_new - 10\n",
    "ymin_new = ymin_new - 20 \n",
    "xmax_new = xmax_new + 10\n",
    "ymax_new = ymax_new + 15\n",
    "\n",
    "image = imgRGB.crop((xmin_new, ymin_new ,xmax_new, ymax_new))\n",
    "\n",
    "transformations = transforms.Compose([transforms.Resize(224), transforms.CenterCrop(224), transforms.ToTensor(),transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "image = transformations(img)\n",
    "\n",
    "\n",
    "image = image.view(1, 3, 224, 224) \n",
    "image = Variable(image).cuda(gpu)\n",
    "x, yaw, pitch, roll = model(image)\n",
    "\n",
    "yaw_predicted = softmax(yaw)#,dim = 1)\n",
    "pitch_predicted = softmax(pitch)#,dim = 1)\n",
    "roll_predicted = softmax(roll)#,dim =1)\n",
    "\n",
    "yaw_predicted = torch.sum(yaw_predicted * idx_tensor, 1) * 3 - 99\n",
    "pitch_predicted = torch.sum(pitch_predicted * idx_tensor, 1) * 3 - 99\n",
    "roll_predicted = torch.sum(roll_predicted * idx_tensor, 1) * 3 - 99\n",
    "\n",
    "yaw_predicted = yaw_predicted.view(1)\n",
    "pitch_predicted = pitch_predicted.view(1)\n",
    "roll_predicted = roll_predicted.view(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import utils\n",
    "\n",
    "#Visualize result\n",
    "utils.draw_axis(frame, yaw_predicted, pitch_predicted, roll_predicted, tdx = frame.shape[1] / 2, tdy= frame.shape[0] / 2, size = frame.shape[1]/2)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(frame)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "053c6e4ee5ae1f0aeeda0edd312d8b4f74168437095e833c07b841acdd0a11a6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
